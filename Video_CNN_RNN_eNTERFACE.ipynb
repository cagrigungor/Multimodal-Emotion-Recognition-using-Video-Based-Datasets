{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video_CNN_RNN_eNTERFACE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Ov46NsAhwF",
        "colab_type": "code",
        "outputId": "7ef4605b-0aea-4a05-8cca-b953de872bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive \n",
        "drive.mount('/mntDrive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /mntDrive; to attempt to forcibly remount, call drive.mount(\"/mntDrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPS_HUT7dqlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eNTERFACE_PATH = \"/mntDrive/My Drive/Data/eNTERFACE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k54tp7ILJ_eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def sorted_alphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
        "    return sorted(data, key=alphanum_key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKSqGY-cAy45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, image_paths,transform):\n",
        "        self.image_paths = image_paths\n",
        "        self.paths = []\n",
        "        self.transform = transform\n",
        "        files = os.listdir(self.image_paths)\n",
        "        tot_num = 0\n",
        "        n = 0\n",
        "\n",
        "        for file in files:\n",
        "            integers = re.findall(r'\\d+', file)\n",
        "            index = int(integers[0]) \n",
        "          \n",
        "            \n",
        "            if index < 32:\n",
        "              self.paths.append(file)\n",
        "            \n",
        "        self.paths = sorted_alphanumeric(self.paths)\n",
        "        #random.shuffle(self.paths)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        path = self.paths[index]\n",
        "        dict_data = np.load(self.image_paths+\"/\"+path)\n",
        "        data = dict_data['arr_0']\n",
        "        data = data[0]\n",
        "        images = []\n",
        "\n",
        "        outdatasize = 16.0\n",
        "        datalen = len(data)\n",
        "        for i in range(int(outdatasize)):\n",
        "          index = math.floor(datalen/outdatasize*(i))\n",
        "  \n",
        "          temp = data[index]\n",
        "          temp = Image.fromarray(temp, 'RGB')\n",
        "          \n",
        "          if self.transform:\n",
        "                temp = self.transform(temp)\n",
        "          #display(temp)\n",
        "          images.append(temp)\n",
        "        x = np.stack(images)\n",
        "        #x = torch.from_numpy(data).float().to(device)\n",
        "      \n",
        "        if \"an\" in path:\n",
        "            y = torch.tensor(0, dtype=torch.long)\n",
        "        elif \"di\" in path:\n",
        "            y = torch.tensor(1, dtype=torch.long)\n",
        "        elif \"fe\" in path:\n",
        "            y = torch.tensor(2, dtype=torch.long)\n",
        "        elif \"ha\" in path:\n",
        "            y = torch.tensor(3, dtype=torch.long)\n",
        "        elif \"sa\" in path:\n",
        "            y = torch.tensor(4, dtype=torch.long)\n",
        "        elif \"su\" in path:\n",
        "            y = torch.tensor(5, dtype=torch.long)\n",
        "        return x,y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def l(self):\n",
        "        return len(self.paths)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEW6VMVCGVxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValDataset(Dataset):\n",
        "    def __init__(self, image_paths,transform):\n",
        "        self.image_paths = image_paths\n",
        "        self.paths = []\n",
        "        self.transform = transform\n",
        "        files = os.listdir(self.image_paths)\n",
        "        tot_num = 0\n",
        "        n = 0\n",
        "\n",
        "        for file in files:\n",
        "            integers = re.findall(r'\\d+', file)\n",
        "            index = int(integers[0]) \n",
        "      \n",
        "            if index > 28:\n",
        "              self.paths.append(file)\n",
        "                \n",
        "        self.paths = sorted_alphanumeric(self.paths)\n",
        "    def __getitem__(self,index):\n",
        "        path = self.paths[index]\n",
        "        dict_data = np.load(self.image_paths+\"/\"+path)\n",
        "        data = dict_data['arr_0']\n",
        "        data = data[0]\n",
        "        images = []\n",
        "\n",
        "        outdatasize = 16.0\n",
        "        datalen = len(data)\n",
        "        for i in range(int(outdatasize)):\n",
        "          index = math.floor(datalen/outdatasize*(i))\n",
        "  \n",
        "          temp = data[index]\n",
        "          temp = Image.fromarray(temp, 'RGB')\n",
        "          if self.transform:\n",
        "                temp = self.transform(temp)\n",
        "          images.append(temp)\n",
        "        x = np.stack(images)\n",
        "      \n",
        "        if \"an\" in path:\n",
        "            y = torch.tensor(0, dtype=torch.long)\n",
        "        elif \"di\" in path:\n",
        "            y = torch.tensor(1, dtype=torch.long)\n",
        "        elif \"fe\" in path:\n",
        "            y = torch.tensor(2, dtype=torch.long)\n",
        "        elif \"ha\" in path:\n",
        "            y = torch.tensor(3, dtype=torch.long)\n",
        "        elif \"sa\" in path:\n",
        "            y = torch.tensor(4, dtype=torch.long)\n",
        "        elif \"su\" in path:\n",
        "            y = torch.tensor(5, dtype=torch.long)\n",
        "        return x,y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def l(self):\n",
        "        return len(self.paths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajH8tYXvG8Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_paths,transform):\n",
        "        self.image_paths = image_paths\n",
        "        self.paths = []\n",
        "        self.transform = transform\n",
        "        files = os.listdir(self.image_paths)\n",
        "        tot_num = 0\n",
        "        n = 0\n",
        "\n",
        "        for file in files:\n",
        "            integers = re.findall(r'\\d+', file)\n",
        "            index = int(integers[0]) \n",
        "  \n",
        "            if index > 32:\n",
        "              if index > 37:\n",
        "                self.paths.append(file)\n",
        "            \n",
        "        self.paths = sorted_alphanumeric(self.paths)\n",
        "        #random.shuffle(self.paths)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        path = self.paths[index]\n",
        "        dict_data = np.load(self.image_paths+\"/\"+path)\n",
        "        data = dict_data['arr_0']\n",
        "        data = data[0]\n",
        "        images = []\n",
        "\n",
        "        outdatasize = 16.0\n",
        "        datalen = len(data)\n",
        "        for i in range(int(outdatasize)):\n",
        "          index = math.floor(datalen/outdatasize*(i))\n",
        "  \n",
        "          temp = data[index]\n",
        "          temp = Image.fromarray(temp, 'RGB')\n",
        "          if self.transform:\n",
        "                temp = self.transform(temp)\n",
        "          images.append(temp)\n",
        "        x = np.stack(images)\n",
        "      \n",
        "        if \"an\" in path:\n",
        "            y = torch.tensor(0, dtype=torch.long)\n",
        "        elif \"di\" in path:\n",
        "            y = torch.tensor(1, dtype=torch.long)\n",
        "        elif \"fe\" in path:\n",
        "            y = torch.tensor(2, dtype=torch.long)\n",
        "        elif \"ha\" in path:\n",
        "            y = torch.tensor(3, dtype=torch.long)\n",
        "        elif \"sa\" in path:\n",
        "            y = torch.tensor(4, dtype=torch.long)\n",
        "        elif \"su\" in path:\n",
        "            y = torch.tensor(5, dtype=torch.long)\n",
        "        return x,y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def l(self):\n",
        "        return len(self.paths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJYDE-eZcxPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = os.listdir(eNTERFACE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cfMKv1lBt2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(226),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "train = TrainDataset(eNTERFACE_PATH,transform)\n",
        "train_loader = DataLoader(train,batch_size=1, shuffle=True)\n",
        "val = ValDataset(eNTERFACE_PATH,transform)\n",
        "val_loader = DataLoader(val,batch_size=1, shuffle=True)\n",
        "test = TestDataset(eNTERFACE_PATH,transform)\n",
        "test_loader = DataLoader(test,batch_size=1, shuffle=True)\n",
        "train_count = train.l()\n",
        "valid_count = val.l()\n",
        "test_count = test.l()\n",
        "\n",
        "#for data, target in train_loader:\n",
        "#    print(data.shape,target[0])\n",
        "    \n",
        "    #if(target[0] == 0):\n",
        "    #  a = a + 1\n",
        "    #if(target[0] == 1):\n",
        "    #  b = b + 1\n",
        "    #print(a,b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VuDBkpe9bV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a3752115-85f9-42a4-df14-da0f52f6ef6d"
      },
      "source": [
        "max_epoch = 50\n",
        "batch_size = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install GPUtil\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Gen RAM Free: 24.7 GB  |     Proc size: 2.7 GB\n",
            "GPU RAM Free: 14597MB | Used: 1683MB | Util  10% | Total     16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izzAEN7L9kY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as weight_init\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self,resnet34): \n",
        "    # you can add any additional parameters you want \n",
        "    # TODO:\n",
        "    # You should create your neural network here\n",
        "        super(ConvNet,self).__init__()\n",
        "      # convolutional layer\n",
        "        self.resnet34 = resnet34\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=256, num_layers=1,  bidirectional=False)\n",
        "        for name, param in self.lstm.named_parameters(): \n",
        "          if 'bias' in name:\n",
        "            weight_init.constant(param, 0.0)\n",
        "          elif 'weight' in name:\n",
        "            weight_init.xavier_uniform(param);\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1,  bidirectional=False)\n",
        "        for name, param in self.lstm2.named_parameters(): \n",
        "          if 'bias' in name:\n",
        "            weight_init.constant(param, 0.0)\n",
        "          elif 'weight' in name:\n",
        "            weight_init.xavier_uniform(param);\n",
        "        #self.fc_lstm=nn.Sequential(\n",
        "        #    self.lstm)\n",
        "        \n",
        "      # fully connected layers\n",
        "        self.fc1 = nn.Linear(1000, 256)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "    \n",
        "          \n",
        "    def forward(self, X): \n",
        "    # you can add any additional parameters you want\n",
        "    # TODO:\n",
        "    # Forward propagation implementation should be here\n",
        "        #X = X[None,:,:,:]\n",
        "        #print(X.shape)\n",
        "        X = self.resnet34(X)\n",
        "        X = self.relu(X)\n",
        "        X = self.fc1(X)\n",
        "        X = X[:,None,:]\n",
        "        X, (h,c) = self.lstm(X)\n",
        "        X, (h,c) = self.lstm2(X)\n",
        "        X = X[-1]\n",
        "        X = self.fc2(X)\n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0LNMkwha8Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(criterion, optimizer, model):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    \n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "    num = 0\n",
        "    # training the model with the train dataset.\n",
        "    for train_images, train_labels in train_loader:\n",
        "        train_images = train_images.to( device ,dtype=torch.float)\n",
        "        train_labels = train_labels.to( device ,dtype=torch.float) \n",
        "\n",
        "      \n",
        "        train_labels_head = model.forward(train_images[0])\n",
        "\n",
        "        loss = criterion(train_labels_head, train_labels.long())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "\n",
        "        predictions = torch.max( train_labels_head , 1)[1] \n",
        "      \n",
        "        acurracy = torch.sum( predictions == train_labels )\n",
        "\n",
        "        accur_save = float(acurracy.float())/1\n",
        "        loss_save= float(loss.item())/1\n",
        "\n",
        "        train_acc += acurracy \n",
        "        train_loss += loss.item()\n",
        "        num = num + 1\n",
        "        if num % 1000 == 0:\n",
        "          print(num,train_acc/num)\n",
        "\n",
        "        train_acc_print = accur_save\n",
        "        batch_acc.append(train_acc_print)\n",
        "        train_loss_print = loss_save\n",
        "        batch_loss.append(train_loss_print)\n",
        "\n",
        "    return train_loss, train_acc,batch_acc,batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPsenUzy9wWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(criterion, model):\n",
        "    valid_loss = 0.0\n",
        "    valid_acc  = 0.0\n",
        "    valid_labels_tot = []\n",
        "    pred = []\n",
        "    # testing the current model within the current epoch.\n",
        "    for valid_images, valid_labels in val_loader:\n",
        "        #GPUs = GPU.getGPUs()\n",
        "        # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "        #gpu = GPUs[0]\n",
        "\n",
        "        #process = psutil.Process(os.getpid())\n",
        "        acurracy,loss,predictions = step(valid_images,valid_labels)\n",
        "        # calculating acurracy for the cur-\n",
        "        # rent batch for the test dataset.\n",
        "        \n",
        "    \n",
        "        # summing test acurracy and \n",
        "        # loss wit the remaining batches.\n",
        "        valid_acc  += acurracy \n",
        "        valid_loss += loss.item()\n",
        "        valid_labels_tot.append(valid_labels)\n",
        "        pred.append(predictions)\n",
        "    return valid_loss, valid_acc,pred,valid_labels_tot\n",
        "def step(valid_images,valid_labels):\n",
        "  # registering test images and labels \n",
        "  # to device to benefit from GPU\n",
        "  valid_images = valid_images.to( device ,dtype=torch.float)\n",
        "  valid_labels = valid_labels.to( device ,dtype=torch.float)\n",
        "  #valid_frame = valid_frame.to( device ,dtype=torch.float)\n",
        "  optimizer.zero_grad()\n",
        "  if valid_images.shape[1] > 120:\n",
        "    temp = int((valid_images.shape[1] - 120) / 2) + 1\n",
        "    tempAr = valid_images[0][temp:-temp]\n",
        "    #print(tempAr.shape)\n",
        "    valid_images = torch.zeros([1,tempAr.shape[0],3,224,224], dtype=torch.float).to( device ,dtype=torch.float)\n",
        "    valid_images[0] = tempAr\n",
        "  # calculating loss for test \n",
        "  # dataset for the current batch.\n",
        "  #if(valid_frame[0][0].to(dtype=torch.int32) != 0):\n",
        "  valid_labels_head = model.forward(valid_images[0])\n",
        "  loss = criterion(valid_labels_head, valid_labels.long())\n",
        "  predictions = torch.max( valid_labels_head, 1)[1] \n",
        "  acurracy = torch.sum( predictions == valid_labels )\n",
        "  \n",
        "  return acurracy,loss,predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yldsa7VR9x43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(criterion, model):\n",
        "    test_loss = 0.0\n",
        "    test_acc  = 0.0\n",
        "    test_labels_tot = []\n",
        "    pred = []\n",
        "    i = 1\n",
        "    # testing the current model within the current epoch.\n",
        "    for test_images, test_labels in val_loader:\n",
        "        \n",
        "        # registering test images and labels \n",
        "        # to device to benefit from GPU\n",
        "        test_images = test_images.to( device ,dtype=torch.float)\n",
        "        test_labels = test_labels.to( device ,dtype=torch.float)\n",
        "        \n",
        "        test_labels_head = model.forward(test_images[0])\n",
        "        loss = criterion(test_labels_head, test_labels.long())\n",
        "        #print(torch.max( test_labels_head, 1)[1] ,test_labels.long())\n",
        "        # calculating acurracy for the cur-\n",
        "        # rent batch for the test dataset.\n",
        "        predictions = torch.max( test_labels_head, 1)[1] \n",
        "        acurracy = torch.sum( predictions == test_labels )\n",
        "        i = i + 1\n",
        "        # summing test acurracy and \n",
        "        # loss wit the remaining batches.\n",
        "        test_acc  += acurracy \n",
        "        print(test_acc/i)\n",
        "        test_loss += loss.item()\n",
        "        test_labels_tot.append(test_labels)\n",
        "        pred.append(predictions)\n",
        "    return test_loss, test_acc, pred, test_labels_tot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3x3gBXF9mkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.init as weight_init\n",
        "\n",
        "resnet34 = torchvision.models.resnet34(pretrained=True)\n",
        "i = 0\n",
        "\n",
        "for name,child in resnet34.named_children(): \n",
        "    for name2,param in child.named_parameters():\n",
        "      if i < 15:\n",
        "        print(name2,i)\n",
        "        i = i + 1\n",
        "        param.requires_grad = False\n",
        "\n",
        "model = ConvNet(resnet34).to(device)\n",
        "#model.load_state_dict(torch.load('/mntDrive/My Drive/Colab Notebooks/cont3.pth'))\n",
        "\n",
        "#model = torch.load('/mntDrive/My Drive/Colab Notebooks/cont3.pth').to(device)\n",
        "\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate) # you can play with momentum and weight_decay parameters as well\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accurracies = []\n",
        "valid_accurracies = []\n",
        "batch_acc_tot  = []\n",
        "batch_loss_tot  = []\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "  \n",
        "    # training the model with the train dataset.\n",
        "    train_loss, train_acc,batch_acc,batch_loss = train_model(criterion, optimizer, model)\n",
        "    batch_acc_tot.append(batch_acc)\n",
        "    batch_loss_tot.append(batch_loss)\n",
        "    # validating the current model within the current epoch.\n",
        "    with torch.no_grad():\n",
        "      valid_loss, valid_acc, predictions, valid_labels = validate_model(criterion, model)\n",
        "\n",
        "    valid_acc = float(valid_acc.float()) / valid_count\n",
        "    valid_accurracies.append( valid_acc )\n",
        "\n",
        "    pred = np.array([])\n",
        "    for i in predictions:\n",
        "      pred = np.concatenate((pred,i.cpu().detach().numpy()),axis=None)\n",
        "\n",
        "    labels = np.array([])\n",
        "    for i in valid_labels:\n",
        "      labels = np.concatenate((labels,i.cpu().detach().numpy()),axis=None)\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    cm = confusion_matrix(labels, pred)\n",
        "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis].astype('float')\n",
        "    \n",
        "    f, ax = plt.subplots(figsize = (10, 10))\n",
        "    sns.heatmap(cmn, annot = True, linewidths=1, linecolor='black', fmt='.2f', ax=ax)\n",
        "    plt.xlabel(\"Test Predictions\")\n",
        "    plt.ylabel(\"Test Labels\")\n",
        "    plt.show()\n",
        "   \n",
        "\n",
        "    print(\"EPOCH --> \" + str(epoch + 1))\n",
        "    print(\"----------------------------------------------\") \n",
        "    train_acc = float(train_acc.float()) / train_count\n",
        "    train_accurracies.append( train_acc )\n",
        "    print(\"Train Acurracy: \" + str( float(train_acc) ))\n",
        "    \n",
        "    train_loss = train_loss / train_count\n",
        "    train_losses.append( train_loss )\n",
        "    print(\"Train Loss: \" + str(train_loss) + \"\\n\")\n",
        "\n",
        "    \n",
        "    print(\"Validation Acurracy: \" + str( float(valid_acc) ))    \n",
        "    \n",
        "    valid_loss = valid_loss / valid_count\n",
        "    valid_losses.append( valid_loss )\n",
        "    print(\"Validation Loss: \" + str(valid_loss))\n",
        "    print(\"----------------------------------------------\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}